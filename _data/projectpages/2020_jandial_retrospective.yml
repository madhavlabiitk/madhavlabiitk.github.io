title: "Retrospective Loss: Looking Back to Improve Training of Deep Neural Networks"
authors: "Surgan Jandial-1, Ayush Chopra-2, Mausoom Sarkar-2, Piyush Gupta-2, Balaji Krishnamurthy-2, Vineeth Balasubramanian-1"
venue: "Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'20)"
affiliations: "1-IIT Hyderabad, 2-Media and Data Science Research Lab Adobe"
display_image: 2020_jandial_retrospective.PNG
abstract: "Deep neural networks (DNNs) are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce a new retrospective loss to improve the training of deep neural network models by utilizing the prior experience available in past model states during training. Minimizing the retrospective loss, along with the task-specific loss, pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. Although a simple idea, we analyze the method as well as to conduct comprehensive sets of experiments across domains - images, speech, text, and graphs - to show that the proposed loss results in improved performance across input domains, tasks, and architectures."
contact: "If you have any questions about this work, please contact us at <span style='color:blue'>cs17btech11038@iith.ac.in</span>"
acknowledgements: "This study was funded by ..."
materials:
  paper_url: https://arxiv.org/abs/2006.13593
  bibtex: "@inproceedings{Jandial2020RetrospectiveLL,
  title={Retrospective Loss: Looking Back to Improve Training of Deep Neural Networks},
  author={Surgan Jandial and Ayush Chopra and Mausoom Sarkar and Piyush Gupta and Balaji Krishnamurthy and Vineeth N. Balasubramanian},
  journal={Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2020}}"
